{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Haiti 2 - Cleaning and Processing Haiti Data</h1>\n",
    "\n",
    "In the last notebook you explored the dataset. In this notebook, you will load in the same [text dataset](https://github.com/rmunro/disaster_response_messages). You'll then clean and process the data using several Natural Language Processing (NLP) techniques.\n",
    "\n",
    "**In this lab you will apply the following steps:**\n",
    "1. Import Python Packages\n",
    "2. Load the data\n",
    "3. Text cleaning and processing\n",
    "4. Explore the number of tokens\n",
    "5. Represent a word as a count-based vector (Bag of Words)\n",
    "6. Explore the top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from termcolor import colored\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import utils\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "training_data = pd.read_csv(\"data/disaster_response_training.csv\", low_memory=False)\n",
    "validation_data = pd.read_csv(\"data/disaster_response_validation.csv\", low_memory=False)\n",
    "test_data = pd.read_csv(\"data/disaster_response_test.csv\", low_memory=False)\n",
    "print('Datasets loaded successfully!')\n",
    "\n",
    "# Merge the three datasets\n",
    "full_data = training_data.append(validation_data).append(test_data)\n",
    "# Fix column data type\n",
    "full_data['original'] = full_data['original'].astype(str)\n",
    "\n",
    "# Select only the Haiti data\n",
    "haiti_df = full_data[full_data.event == 'haiti_earthquake']\n",
    "# Fix column data type\n",
    "haiti_df['actionable_haiti'] = haiti_df.actionable_haiti.astype('int64')\n",
    "haiti_df['date_haiti'] = pd.to_datetime(haiti_df.date_haiti)\n",
    "print('Haiti data selected!')\n",
    "haiti_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text cleaning and processing\n",
    "\n",
    "Before you can perform any modelling on the text, you need to go through a series of steps to clean and process the data. The steps include\n",
    "\n",
    "1. **Tokenize:** This splits strings based on white spaces and punctuation. Further, it expands contractions (i.e `can't` becomes `ca` and `n't`\n",
    "2. **Remove puncutation:** To remove the puncuations from words, you will use `string.punctuation` which is a list of all punctuation symbols. You'll notice that the list is not exhaustive and you may need to add additional punctuations based on your specific dataset (check the `utils` file for details).\n",
    "3. **Standardize letter case:** You also want to ensure that all words follow the same format in order to recognize duplicate words in a message. To do this, you'll convert all words to lowercase. \n",
    "4. **Remove stop words:** These are common words that are often used in speech or text that you may not want to include in our final analysis, as words like \"and\" are very common and may skew the results of what you're trying to analyze.\n",
    "5. **Lemmatize each word:** In our case, you want to know what general topics are being spoken about in relation to the Haiti earthquake, and so it doesn't matter as much to us whether someone mentions \"help\", \"helped\", or \"helping\". Lemmatization is not the best solution for every kind of task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clean and process a single message\n",
    "Before you process the entire dataset, let's look at a random message and see how each step cleans and processes each word in the message. In the below cell you see the code with each of the steps. Then in the next cell you can run it in an interactive mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords and punctuation from the utils file\n",
    "STOP_WORDS = utils.STOP_WORDS\n",
    "punctuation = utils.punctuation\n",
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def process_random_message(df):\n",
    "    message = utils.get_random_message(df)\n",
    "    \n",
    "    # Step 1: Tokenize and get POS tags\n",
    "    tokens = nltk.pos_tag(word_tokenize(message))\n",
    "    print(colored(\"Step 1: Tokenize\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "    \n",
    "    # Step 2: Standardize Lettercase\n",
    "    tokens = [(w[0].lower(), w[1]) for w in tokens]\n",
    "    print(colored(\"Step 2: Lowercase\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 3: Remove Puncuation\n",
    "    tokens = [w for w in tokens if w[0] not in punctuation]\n",
    "    print(colored(\"Step 3: Remove punctuation\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 4: Remove stop words\n",
    "    tokens = [w for w in tokens if w[0] not in STOP_WORDS]\n",
    "    print(colored(\"Step 5: Remove stop words\\n\", \"blue\"), \"{}\\n\".format([w[0] for w in tokens]))\n",
    "\n",
    "    # Step 5: Lemmatize each word \n",
    "    tokens = [lemmatizer.lemmatize(w[0], utils.pos_tag_convert(w[1])) for w in tokens]\n",
    "    print(colored(\"Step 4: Lemmatize\\n\", \"blue\"), \"{}\\n\".format(tokens))\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.interact_with_filters(process_random_message, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3.2 Process the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can put all of the following cleaning together into one method (defined as `clean_tokenize_process_text` in the `utils` file). Now you can run it for the whole dataframe and save the tokenized messages to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all messages and save tokens to a new column\n",
    "haiti_df[\"message_tokens\"] = haiti_df.message.apply(\n",
    "    utils.process_text,\n",
    "    tokenizer=word_tokenize, pos_tagger=nltk.pos_tag, lemmatizer=lemmatizer, stopwords=STOP_WORDS, punctuation=punctuation\n",
    ")\n",
    "\n",
    "haiti_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now look at some of the messages and their tokens directly from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utils.interact_with_filters(utils.print_messages, haiti_df, number_of_messages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Explore the number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Count the number of words and tokens\n",
    "What is the distribution of the length of each message? You can calculate that and use a histogram to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Count number of words and save as a column\n",
    "haiti_df[\"num_words\"] = haiti_df.message.apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# Count number of tokens and save as a column\n",
    "haiti_df[\"num_tokens\"] = haiti_df.message_tokens.apply(lambda x: len(x))\n",
    "\n",
    "utils.interact_with_filters(utils.histogram_number_of_words, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Several of your messages are quite short. Let's see which messages have only 1 token after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "haiti_df[haiti_df.num_tokens==1][['message', 'message_tokens']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, this helps us to find which messages may be anomalous or strange. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Represent a word as a count-based vector (Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Explore a mini corpus\n",
    "To make things easy to visualize and comprehend, you can look at a small corpus consisting of just a few messages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.interact_with_filters(\n",
    "    utils.mini_corpus, haiti_df,\n",
    "    corpus_size=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Apply BoW to full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the messages in the dataset\n",
    "corpus = haiti_df.message_tokens\n",
    "\n",
    "# Create the Dictionary\n",
    "corpus_dictionary = gensim.corpora.Dictionary(corpus)\n",
    "\n",
    "print(colored(\"Before filter: \", \"blue\"), f\"Dictionary contains a total of {len(corpus_dictionary)} unique words\")\n",
    "\n",
    "# Filter for words that occur more than 5 times overall\n",
    "corpus_dictionary.filter_extremes(no_below=5)\n",
    "\n",
    "# Create bag of words out of the full corpus\n",
    "corpus_bow = [corpus_dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "print(colored(\"After filter: \", \"blue\"), f\"Dictionary contains a total of {len(corpus_dictionary)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore the top words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can look at the top words that are used overall in our texts. You can do this by combining all of our messages into one long document, then you can see what are the top most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.interact_with_filters(utils.explore_top_tokens, haiti_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a visualization tool such as a word cloud to plot the top words used in all of the messages overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils.interact_with_filters(utils.wordcloud_from_top_words, haiti_df, n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.interact_with_filters(utils.relative_words_visualization, haiti_df, n=50, show_other=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the updated dataframe as a python pickle below. The pickle module is used for serializing and de-serializing a Python object structure. This means you can save any object from python and open it next time exactly the same as it was. If you save it as .csv for example some information about column types may be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open('haiti_df.pkl', 'wb') as f:\n",
    "#    pickle.dump(haiti_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation:\n",
    "- Robert Munro. 2012. Processing short message communications in low-resource languages. [PhD dissertation, Stanford University]. Stanford Digital Repository. Retrieved from https://purl.stanford.edu/cg721hb0673"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
