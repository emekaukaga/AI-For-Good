{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurricane Harvey: Explore the Data\n",
    "\n",
    "In this notebook, you will explore the Hurricane Harvey image dataset. The dataset you will be looking at came from [Detecting Damaged Buildings on Post-Hurricane Satellite Imagery Based on Customized Convolutional Neural Networks](https://dx.doi.org/10.21227/sdad-1e56) and was prepared by Quoc Dung Cao and Youngjun Choe on the 13th of December, 2018. [This paper](https://arxiv.org/pdf/1807.01688.pdf) describes the authors' work in more detail and more [information about the original satellite image data source can be found here](https://www.satimagingcorp.com/satellite-sensors/geoeye-1/). Individual images were labeled by volunteers on the [crowdsourcing platform Tomnod, now Maxar Geohive](https://blog.maxar.com/leading-the-industry/2019/in-the-blink-of-an-eye-looking-back-on-nine-years-with-tomnod). This dataset contains aerial-view windows of the affected area classified as damaged or not, along with building coordinates. \n",
    "\n",
    "In this lab you will apply the following steps:\n",
    " 1. Import Python packages\n",
    " 2. Explore the dataset\n",
    " 3. Prepare the dataset\n",
    " 4. Describe the data\n",
    " 5. Compare pre-disaster and post-disaster images\n",
    " 6. Visualizing classified locations with LeafLet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Python packages\n",
    "\n",
    "Run the next cell to import the Python packages you'll be using in this lab exercise. If everything goes well you should see a message when the cell has finished running that says \"All packages imported successfully!\".\n",
    "\n",
    "Note the `import utils` line. This line imports the functions that were specifically written for this lab. If you want to look at what these functions are, go to `File -> Open...` and open the `utils.py` file to have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import utils\n",
    "\n",
    "print('All packages imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to display any image and its location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "components = utils.images_on_server(display)\n",
    "\n",
    "display(components['fileChooser'])\n",
    "display(components['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to represent the image dataset in a dataframe using the `pandas` package. A dataframe is just a convenient format you can use for accessing and manipulating data. We will only use this dataframe to understand the path of each image and highlight key features in the naming conventions of the folders and locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has already been split for you into the `train`, `validation`, and `test`. The path name of each image gives us important information. \n",
    "\n",
    "```\n",
    "|- data\n",
    "  |- train\n",
    "    |- visible_damage\n",
    "    |- no_damage\n",
    "  |- validation\n",
    "    |- visible_damage\n",
    "    |- no_damage\n",
    "  |- test\n",
    "    |- visible_damage\n",
    "    |- no_damage\n",
    "```\n",
    "\n",
    "* `data`: root folder containing all images\n",
    "  * `train`: set of data used to train the model\n",
    "  * `validation`: set of data used to fine tune the model during training\n",
    "  * `test`: set of data used to test the performance of the model\n",
    "     * `visible_damage`: set of images that display evidence of damage\n",
    "     * `no_damage`: set of images that do not display evidence of damage\n",
    "\n",
    "Each image within each folder is given a unique name containing two numbers. The first number refers to the **longitude** coodinates of the satellite image and the second number refers to the **latitude** coodinates.  \n",
    "\n",
    "**For example:** \n",
    "\n",
    "`/train/visible_damage/-95.638338_29.771164000000002.jpeg`\n",
    "* `train`: This image is a part of the training set\n",
    "* `visible_damage`: This image displays evidence of damage\n",
    "* `-95.638338`: Longitude coordinates of the image\n",
    "* `29.771164000000002`: Latitude coodinates of the image\n",
    "* `.jpeg`: Type of image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata = utils.get_dataframe_from_file_structure()\n",
    "\n",
    "# Display the first five rows of the dataset\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4. Describe the data\n",
    "\n",
    "Let's look at how many images we have for each of the categoreis (damage and no damage) for each of the three splits (train, val, and test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Understand image distribution across dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe that summarizes image classification in each subset\n",
    "df = pd.pivot_table(metadata, index='subset', columns='label', values='filename', aggfunc='count')\n",
    "\n",
    "# Add new column with total number of images in each subset\n",
    "df['total'] = df['visible_damage'] + df['no_damage']\n",
    "\n",
    "# Show dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Compare pre-disaster and post-disaster images\n",
    "\n",
    "You can plot different images before and after the disaster and check how the affected buildings look. You can use the widget to look at different pairs of images. The `Image Num` parameter corresponds to the index number for each image in the no damage training directory.\n",
    "\n",
    "**Note** In the labs that follow, you'll be building a classifier to assess images directly for the presence of visible damage, which is to say, you won't be training an algorithm to recognize differences between images taken before and after a disaster. However, doing a before an after comparison is one approach to doing damage assessment and a number of teams around the world have built such systems. Through these labs, you'll have several opportunities to look at before and after images as a way of visualizing how damage appears in the images and it's worth also thinking about how you might design a different type of system to recognize damage given before and after pairs of images. In many cases, you may not have access to images taken before the disaster, but even if you do, you would need to potentially deal with issues of alignment on a pixel-by-pixel basis between images taken at different times, as well as color variations due to different lighting and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'train'\n",
    "# Match images in the no damage and damage dataset based on the location coordinates in the file name\n",
    "matches = list(set(metadata.query(f'subset == \"{dataset}\" & label == \"visible_damage\"')['filename'])\n",
    "               .intersection(metadata.query(f'subset == \"{dataset}\" & label == \"no_damage\"')['filename']))\n",
    "\n",
    "# Load index slider to navigate between the paired images\n",
    "file_index_widget = widgets.IntSlider(min=0, max=len(matches)-1, value=10, description='Image Num')\n",
    "\n",
    "# Load visualizer to match paired images\n",
    "interact(utils.interactive_plot_pair(f'./data/{dataset}/', matches), file_index=file_index_widget);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you toggle through the images, reflect on the following questions:\n",
    "* Consider what types of damage can you identify in the pairs of images?\n",
    "* What indicators (color, brightness, sharpness, etc.) identify the existance of damage in the image?\n",
    "* What are some restrictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing classified locations with LeafLet\n",
    "\n",
    "Plot the train data using LeafLet/OpenStreetMaps services. It is a way to visualize your data using interactive maps. The pins showing locations of images containing damage are colored in red and the ones without damage are colored green. Clicking on the pin, you will also see the image from that location (it may take a few seconds to show). The map could take 3 minutes to render."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Leadlet visualization with coordinates and labels\n",
    "utils.leaflet_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
